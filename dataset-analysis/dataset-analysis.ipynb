{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0189ce9c-0bca-4aeb-bc6d-c9a01cbb8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f157f-a8bd-4b86-b7e6-5025376194af",
   "metadata": {},
   "source": [
    "# DialogSum\n",
    "\n",
    "## Description\n",
    "DialogSum dataset was created using various dialogue datasets. The dataset merged the source datasets with a common format. There may exist more than 2 people in the dialogues. In order to use the dataset for a chatbot, we have some constraints beforehand:\n",
    "\n",
    "1. A dialogue has only 2 people.\n",
    "1. A dialogue can be made up of any daily life conversation.\n",
    "1. A dialogue may consists of technical terms, only if the dialogue can be used in a daily conversatiob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4032477-3a75-4f9d-85d6-cd8a615e56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration knkarthick--dialogsum-cd575843ad07bb63\n",
      "Found cached dataset csv (/Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd575843ad07bb63/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "    num_rows: 12460\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'knkarthick/dialogsum'\n",
    "trainset = load_dataset(dataset, split='train')\n",
    "\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bc528b-4bed-4414-9b18-639b2e13273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd575843ad07bb63/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-5a300de32c77547b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_func(x):\n",
    "    dialogue = x['dialogue']\n",
    "    persons = set(filter(lambda x: '#person' in x, dialogue.lower().split()))\n",
    "\n",
    "    return len(persons) == 2\n",
    "\n",
    "trainset = trainset.filter(filter_func)\n",
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4ea6e-cc0c-428b-bbfb-bcff94621d07",
   "metadata": {},
   "source": [
    "### Dialogue Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d337085f-3b64-495b-9c57-99ee5446cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd575843ad07bb63/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-03973742634ac395.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic', 'Person1 Char Avg', 'Person1 Char Max', 'Person2 Char Avg', 'Person2 Char Max', 'Total Char Avg', 'Total Char Max', 'Person1 Word Avg', 'Person1 Word Max', 'Person2 Word Avg', 'Person2 Word Max', 'Total Word Avg', 'Total Word Max'],\n",
       "    num_rows: 12333\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_length(x):\n",
    "    dialog = x['dialogue']\n",
    "    dialog = re.sub(\"\\n\", \"\", dialog)\n",
    "    dialogs = re.split('#Person[\\d]#: ', dialog)[1:]\n",
    "    \n",
    "    char_length = list(map(len, dialogs))\n",
    "    \n",
    "    person1 = char_length[::2]\n",
    "    person2 = char_length[1::2]\n",
    "    \n",
    "\n",
    "    x['Person1 Char Avg'] = sum(person1) / len(person1)\n",
    "    x['Person1 Char Max'] = max(person1)\n",
    "    \n",
    "    x['Person2 Char Avg'] = sum(person2) / len(person2)\n",
    "    x['Person2 Char Max'] = max(person2)\n",
    "    \n",
    "    x['Total Char Avg'] = sum(char_length) / len(char_length)\n",
    "    x['Total Char Max'] = max(char_length)\n",
    "    \n",
    "    word_count = list(map(lambda x: x.count(' ')+1, dialogs))\n",
    "    \n",
    "    person1 = word_count[::2]\n",
    "    person2 = word_count[1::2]\n",
    "    \n",
    "    x['Person1 Word Avg'] = sum(person1) / len(person1)\n",
    "    x['Person1 Word Max'] = max(person1)\n",
    "    \n",
    "    x['Person2 Word Avg'] = sum(person2) / len(person2)\n",
    "    x['Person2 Word Max'] = max(person2)\n",
    "    \n",
    "    x['Total Word Avg'] = sum(word_count) / len(word_count)\n",
    "    x['Total Word Max'] = max(word_count)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "trainset = trainset.map(analyze_length)\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc7fd08-6994-4187-8277-c8819ec623d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_fn at 0x13711e3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01ba34c2ba4740b3d5b614ec22397b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12333 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': 'hi mr smith doctor hawkins today found would good idea get check yes well one 5 years one every year know figure long nothing wrong go see doctor well best way avoid serious illnesses find early try come least year good ok let see eyes ears look fine take deep breath please smoke mr smith yes smoking leading cause lung cancer heart disease know really quit tried hundreds times seem kick habit well classes medications might help give information leave ok thanks doctor',\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up',\n",
       " 'Person1 Char Avg': 94.83333333333333,\n",
       " 'Person1 Char Max': 133,\n",
       " 'Person2 Char Avg': 36.5,\n",
       " 'Person2 Char Max': 74,\n",
       " 'Total Char Avg': 65.66666666666667,\n",
       " 'Total Char Max': 133,\n",
       " 'Person1 Word Avg': 18.166666666666668,\n",
       " 'Person1 Word Max': 28,\n",
       " 'Person2 Word Avg': 7.833333333333333,\n",
       " 'Person2 Word Max': 16,\n",
       " 'Total Word Avg': 13.0,\n",
       " 'Total Word Max': 28}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_fn(x):\n",
    "    # preprocess_fn removes the non-alphanumeric symbols and stopwords\n",
    "    # stopwords can be found in nltk english stopwords\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    non_punctuation = re.compile(r'[^A-Za-z0-9\\s]')\n",
    "    stops = set([re.sub(non_punctuation, ' ', x) for x in stopwords.words('english')])\n",
    "    \n",
    "    dialogue = x['dialogue']\n",
    "\n",
    "    dialogue = re.sub('[^A-Za-z0-9\\s]', ' ', dialogue)\n",
    "    dialogue = re.sub('Person[\\d]', '', dialogue)\n",
    "    dialogue = ' '.join(list(filter(lambda x: x not in stops, dialogue.lower().split())))\n",
    "    \n",
    "    x['dialogue'] = dialogue\n",
    "    return x\n",
    "    \n",
    "trainset = trainset.map(preprocess_fn)\n",
    "\n",
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e15d1b-922c-402e-a9a0-83b7ff34ea66",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b894b71-9d4b-42eb-b446-efbc38705d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 8794),\n",
       " ('yes', 7548),\n",
       " ('well', 7462),\n",
       " ('think', 5981),\n",
       " ('good', 5969),\n",
       " ('know', 5726),\n",
       " ('get', 5450),\n",
       " ('go', 5320),\n",
       " ('would', 4919),\n",
       " ('one', 4883),\n",
       " ('see', 4799),\n",
       " ('really', 4699),\n",
       " ('oh', 4685),\n",
       " ('time', 4582),\n",
       " ('right', 4379),\n",
       " ('want', 4227),\n",
       " ('going', 3825),\n",
       " ('take', 3620),\n",
       " ('much', 3579),\n",
       " ('need', 3365),\n",
       " ('ok', 3295),\n",
       " ('let', 3128),\n",
       " ('please', 3118),\n",
       " ('sure', 3097),\n",
       " ('could', 2834),\n",
       " ('work', 2651),\n",
       " ('people', 2629),\n",
       " ('thank', 2608),\n",
       " ('look', 2515),\n",
       " ('got', 2489)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs = defaultdict(lambda: 0)\n",
    "\n",
    "for dialog in trainset['dialogue']:\n",
    "    for word in dialog.split():\n",
    "        word_freqs[word] += 1\n",
    "    \n",
    "most_freq = sorted(list((k, v) for k, v in word_freqs.items()), key=lambda x: x[1], reverse=True)\n",
    "most_freq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff9cda-3b5f-471c-9da1-4e407661bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
