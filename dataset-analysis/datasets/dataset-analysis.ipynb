{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0189ce9c-0bca-4aeb-bc6d-c9a01cbb8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, deto\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f157f-a8bd-4b86-b7e6-5025376194af",
   "metadata": {},
   "source": [
    "# DialogSum\n",
    "\n",
    "## Description\n",
    "DialogSum dataset was created using various dialogue datasets. The dataset merged the source datasets with a common format. There may exist more than 2 people in the dialogues. In order to use the dataset for a chatbot, we have some constraints beforehand:\n",
    "\n",
    "1. A dialogue has only 2 people.\n",
    "1. A dialogue can be made up of any daily life conversation.\n",
    "1. A dialogue may consists of technical terms, only if the dialogue can be used in a daily conversatiob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4032477-3a75-4f9d-85d6-cd8a615e56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration knkarthick--dialogsum-cd575843ad07bb63\n",
      "Found cached dataset csv (/Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd575843ad07bb63/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "    num_rows: 12460\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'knkarthick/dialogsum'\n",
    "trainset = load_dataset(dataset, split='train')\n",
    "\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5bc528b-4bed-4414-9b18-639b2e13273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd575843ad07bb63/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-5a300de32c77547b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_func(x):\n",
    "    dialogue = x['dialogue']\n",
    "    persons = set(filter(lambda x: '#person' in x, dialogue.lower().split()))\n",
    "\n",
    "    return len(persons) == 2\n",
    "\n",
    "trainset = trainset.filter(filter_func)\n",
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4ea6e-cc0c-428b-bbfb-bcff94621d07",
   "metadata": {},
   "source": [
    "### Dialogue Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d337085f-3b64-495b-9c57-99ee5446cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd575843ad07bb63/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-777551ed5820dafe.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic', 'Person1 Char Avg', 'Person1 Char Max', 'Person2 Char Avg', 'Person2 Char Max', 'Total Char Avg', 'Total Char Max', 'Person1 Word Avg', 'Person1 Word Max', 'Person2 Word Avg', 'Person2 Word Max', 'Total Word Avg', 'Total Word Max'],\n",
       "    num_rows: 12333\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_length(x):\n",
    "    dialog = x['dialogue']\n",
    "    dialog = re.sub(\"\\n\", \"\", dialog)\n",
    "    dialogs = re.split('#Person[\\d]#: ', dialog)[1:]\n",
    "    \n",
    "    char_length = list(map(len, dialogs))\n",
    "    \n",
    "    person1 = char_length[::2]\n",
    "    person2 = char_length[1::2]\n",
    "    \n",
    "\n",
    "    x['Person1 Char Avg'] = sum(person1) / len(person1)\n",
    "    x['Person1 Char Max'] = max(person1)\n",
    "    \n",
    "    x['Person2 Char Avg'] = sum(person2) / len(person2)\n",
    "    x['Person2 Char Max'] = max(person2)\n",
    "    \n",
    "    x['Total Char Avg'] = sum(char_length) / len(char_length)\n",
    "    x['Total Char Max'] = max(char_length)\n",
    "    \n",
    "    word_count = list(map(lambda x: x.count(' ')+1, dialogs))\n",
    "    \n",
    "    person1 = word_count[::2]\n",
    "    person2 = word_count[1::2]\n",
    "    \n",
    "    x['Person1 Word Avg'] = sum(person1) / len(person1)\n",
    "    x['Person1 Word Max'] = max(person1)\n",
    "    \n",
    "    x['Person2 Word Avg'] = sum(person2) / len(person2)\n",
    "    x['Person2 Word Max'] = max(person2)\n",
    "    \n",
    "    x['Total Word Avg'] = sum(word_count) / len(word_count)\n",
    "    x['Total Word Max'] = max(word_count)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "trainset = trainset.map(analyze_length)\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7fd08-6994-4187-8277-c8819ec623d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e7c6176e524a66a21623ce100358ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12333 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Person1', '#', ':', 'Hi', ',', 'Mr.', 'Smith', '.', \"'m\", 'Doctor', 'Hawkins', '.', 'today', '?', '#', 'Person2', '#', ':', 'found', 'would', 'good', 'idea', 'get', 'check-up', '.', '#', 'Person1', '#', ':', 'Yes', ',', 'well', ',', \"n't\", 'one', '5', 'years', '.', 'one', 'every', 'year', '.', '#', 'Person2', '#', ':', 'know', '.', 'figure', 'long', 'nothing', 'wrong', ',', 'go', 'see', 'doctor', '?', '#', 'Person1', '#', ':', 'Well', ',', 'best', 'way', 'avoid', 'serious', 'illnesses', 'find', 'early', '.', 'try', 'come', 'least', 'year', 'good', '.', '#', 'Person2', '#', ':', 'Ok.', '#', 'Person1', '#', ':', 'Let', 'see', '.', 'eyes', 'ears', 'look', 'fine', '.', 'Take', 'deep', 'breath', ',', 'please', '.', 'smoke', ',', 'Mr.', 'Smith', '?', '#', 'Person2', '#', ':', 'Yes', '.', '#', 'Person1', '#', ':', 'Smoking', 'leading', 'cause', 'lung', 'cancer', 'heart', 'disease', ',', 'know', '.', 'really', 'quit', '.', '#', 'Person2', '#', ':', \"'ve\", 'tried', 'hundreds', 'times', ',', 'ca', \"n't\", 'seem', 'kick', 'habit', '.', '#', 'Person1', '#', ':', 'Well', ',', 'classes', 'medications', 'might', 'help', '.', \"'ll\", 'give', 'information', 'leave', '.', '#', 'Person2', '#', ':', 'Ok', ',', 'thanks', 'doctor', '.']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_fn(x):\n",
    "    # preprocess_fn removes the non-alphanumeric symbols and stopwords\n",
    "    # stopwords can be found in nltk english stopwords\n",
    "    \n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    dialogue = x['dialogue']\n",
    "        \n",
    "    print(list(filter(lambda x: x.lower() not in stops, word_tokenize(dialogue))))\n",
    "    #dialogue = ' '.join(list(filter(lambda x: x.lower() not in stops, dialogue.split())))\n",
    "\n",
    "    input()\n",
    "    dialogue = re.sub('[^A-Za-z0-9\\s]', '', dialogue)\n",
    "    dialogue = re.sub('\\s', ' ', dialogue)\n",
    "    x['dialogue'] = dialogue\n",
    "    return x\n",
    "    \n",
    "trainset = trainset.map(preprocess_fn)\n",
    "\n",
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60c6150-98ab-4e2d-b0cb-225f1464ec0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e15d1b-922c-402e-a9a0-83b7ff34ea66",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b894b71-9d4b-42eb-b446-efbc38705d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person1', 60690),\n",
       " ('person2', 55892),\n",
       " ('like', 8779),\n",
       " ('well', 8169),\n",
       " ('im', 8121),\n",
       " ('yes', 7520),\n",
       " ('think', 5975),\n",
       " ('good', 5886),\n",
       " ('know', 5715),\n",
       " ('get', 5436),\n",
       " ('thats', 5324),\n",
       " ('go', 5308),\n",
       " ('would', 4908),\n",
       " ('see', 4788),\n",
       " ('one', 4713),\n",
       " ('really', 4695),\n",
       " ('oh', 4644),\n",
       " ('time', 4397),\n",
       " ('right', 4358),\n",
       " ('want', 4226),\n",
       " ('going', 3809),\n",
       " ('dont', 3789),\n",
       " ('ill', 3739),\n",
       " ('take', 3600),\n",
       " ('much', 3577),\n",
       " ('need', 3365),\n",
       " ('ok', 3293),\n",
       " ('please', 3110),\n",
       " ('sure', 3095),\n",
       " ('could', 2822)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs = defaultdict(lambda: 0)\n",
    "\n",
    "for dialog in trainset['dialogue']:\n",
    "    for word in dialog.split():\n",
    "        word_freqs[word] += 1\n",
    "    \n",
    "most_freq = sorted(list((k, v) for k, v in word_freqs.items()), key=lambda x: x[1], reverse=True)\n",
    "most_freq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9536b36-391a-4a2a-880b-da091540bebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff9cda-3b5f-471c-9da1-4e407661bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
