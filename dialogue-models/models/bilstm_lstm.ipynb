{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import *\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02a7f4f08144f7d8d7d60bc70166c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|UNK|\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n\u001b[0;32m---> 58\u001b[0m lm_train \u001b[38;5;241m=\u001b[39m \u001b[43mQADataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mQADataset.__init__\u001b[0;34m(self, dataset, tokenizer, vocab, maxlen, special_tokens, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens \u001b[38;5;241m=\u001b[39m special_tokens\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vocab\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sentence_pairs)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tensor(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mQADataset.get_vocab\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     49\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sentence_pairs)\n\u001b[1;32m     51\u001b[0m flatten_tokens \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|UNK|\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dialogue38/lib/python3.8/site-packages/torchtext/vocab/vocab_factory.py:99\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     97\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# First sort by descending frequency, then lexicographically\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dialogue38/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, vocab, maxlen, special_tokens, device):\n",
    "        super(QADataset, self).__init__()\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "        self.tokenizer = get_tokenizer('spacy') if tokenizer is None else tokenizer\n",
    "        self.vocab = self.get_vocab(dataset) if vocab is None else vocab\n",
    "\n",
    "        dataset = dataset.map(self.create_sentence_pairs)\n",
    "        self.questions = self.get_tensor(dataset['questions'])\n",
    "        self.answers = self.get_tensor(dataset['answers'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.questions[i], self.answers[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def create_sentence_pairs(self, x):\n",
    "        sentences = re.split(r'[\\s]*#Person\\d#: ', x['dialogue'])[1:]\n",
    "\n",
    "        first_sents = []\n",
    "        second_sents = []\n",
    "        for sent1, sent2 in zip(sentences[:-1], sentences[1:]):\n",
    "            sent1 = self.tokenize_sent(sent1)\n",
    "            sent2 = self.tokenize_sent(sent2)\n",
    "            \n",
    "            first_sents.append(sent1)\n",
    "            second_sents.append(sent2)\n",
    "\n",
    "        return {'questions': first_sents, 'answers': second_sents}\n",
    "\n",
    "    def tokenize_sent(self, sent):\n",
    "        sent = ' '.join([self.special_tokens['bos_token'], sent, self.special_tokens['eos_token']])\n",
    "        sent = [tok for tok in self.tokenizer(sent)]\n",
    "        sent = sent[:self.maxlen]\n",
    "        sent = sent + [self.special_tokens['eos_token']] * (self.maxlen - len(sent))\n",
    "        return sent\n",
    "    \n",
    "    def get_tensor(self, sents):\n",
    "        tokens = torch.zeros((len(sent), self.maxlen), dtype=torch.long, device=device)\n",
    "        for i, sent in enumerate(flatten_tokens):\n",
    "            tokens[i, :] = torch.tensor(self.vocab(sent))\n",
    "        return tokens\n",
    "    \n",
    "    def get_vocab(self, dataset):\n",
    "        dataset = dataset.map(self.create_sentence_pairs)\n",
    "                \n",
    "        flatten_tokens = dataset['questions'] + dataset['answers']\n",
    "        vocab = build_vocab_from_iterator(flatten_tokens, min_freq=5,\n",
    "                                          specials=list(self.special_tokens.values()))\n",
    "        vocab.set_default_index(vocab['|UNK|'])\n",
    "\n",
    "        return vocab\n",
    "\n",
    "lm_train = QADataset(train_dataset, None, None, 100, special_tokens, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstmModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, n_layers=32, n_hidden=128, dropout_rate=0.2):\n",
    "        super(LstmModel, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, embedding_dim=self.embed_dim)\n",
    "        self.lstm = nn.LSTM(self.embed_dim, self.n_hidden, num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, states=None):\n",
    "        out = self.dropout(self.embed(x))\n",
    "        out, states = self.lstm(out, states)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out, states\n",
    "\n",
    "    def init_states(self, batch_size, device):\n",
    "        h = torch.zeros((self.n_layers, batch_size, self.n_hidden), device=device)\n",
    "        c = torch.zeros((self.n_layers, batch_size, self.n_hidden), device=device)\n",
    "\n",
    "        return (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    batch_losses = []\n",
    "\n",
    "    for i, batch in enumerate(pbar):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        states = model.init_states(batch.size(0), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        states = [state.detach() for state in states]\n",
    "        y_pred, states = model(batch[:, :-1], states)\n",
    "        loss = criterion(y_pred.moveaxis(1, -1), batch[:, 1:])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "        pbar.set_description(f'Batch Loss: {loss.item():.3f} Train Loss: {np.mean(batch_losses):.3f}')\n",
    "\n",
    "    return np.mean(batch_losses)\n",
    "\n",
    "\n",
    "def eval_step(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    batch_losses = []\n",
    "\n",
    "    for i, batch in enumerate(pbar):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        y_pred, _ = model(batch[:, :-1])\n",
    "        loss = criterion(y_pred.moveaxis(1, -1), batch[:, 1:])\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "        pbar.set_description(f'Batch Loss: {loss.item():.3f} Validation Loss: {np.mean(batch_losses):.3f}')\n",
    "\n",
    "    return np.mean(batch_losses)\n",
    "\n",
    "\n",
    "def answer(model, sent, tokenizer, vocab, maxlen, special_tokens, device):\n",
    "    model.eval()\n",
    "\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "    sent = ' '.join([special_tokens['bos_token'], sent, special_tokens['pad_token']])\n",
    "    sent = vocab(tokenizer(sent))\n",
    "    sent = torch.tensor(sent, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred, states = model(sent)\n",
    "\n",
    "        pred_tokens = y_pred.argmax(dim=-1, keepdim=True)\n",
    "        # sent_preds = vocab.lookup_tokens(list(pred_tokens))\n",
    "\n",
    "        answer = []\n",
    "        for j in range(maxlen - len(sent)):\n",
    "            last_idx = pred_tokens[-1]\n",
    "            answer.append(vocab.lookup_token(last_idx))\n",
    "\n",
    "            if answer[-1] == special_tokens['eos_token']:\n",
    "                break\n",
    "\n",
    "            y_pred, states = model(last_idx, states)\n",
    "            pred_tokens = y_pred.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        return detokenizer.detokenize(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_len, epochs, bs, lr, embed_dim, n_layers, n_hidden, device):\n",
    "    special_tokens = {'bos_token': '|BOS|',\n",
    "                      'pad_token': '|PAD|',\n",
    "                      'eos_token': '|EOS|',\n",
    "                      'unk_token': '|UNK|'}\n",
    "\n",
    "    train_dataset = load_dataset('knkarthick/dialogsum', split='train')\n",
    "    val_dataset = load_dataset('knkarthick/dialogsum', split='validation')\n",
    "\n",
    "    lm_train = LMDataset(train_dataset, None, None, max_len, special_tokens, device)\n",
    "    lm_valid = LMDataset(val_dataset, lm_train.tokenizer, lm_train.vocab, max_len, special_tokens, device)\n",
    "\n",
    "    train_loader = DataLoader(lm_train, batch_size=bs, shuffle=True)\n",
    "    val_loader = DataLoader(lm_valid, batch_size=bs)\n",
    "\n",
    "    model = LstmModel(len(lm_train.vocab), embed_dim=embed_dim, n_layers=n_layers, n_hidden=n_hidden, dropout_rate=0.1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_loss = train_step(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = eval_step(model, val_loader, criterion, device)\n",
    "    print(answer(model, 'Hi, how are you?', lm_train.tokenizer, lm_train.vocab, max_len, special_tokens, device))\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration knkarthick--dialogsum-caf2f3e75d9073aa\n",
      "Found cached dataset csv (/Users/bugrahamzagundog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-caf2f3e75d9073aa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/dialogue38/lib/python3.8/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0561ce95714f85872c5a1e0abe5682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34829d92a4a94b22aea39c78f18ad96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbos_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|BOS|\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|PAD|\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meos_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|EOS|\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munk_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|UNK|\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknkarthick/dialogsum\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m lm_train \u001b[38;5;241m=\u001b[39m \u001b[43mQADataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mQADataset.__init__\u001b[0;34m(self, dataset, tokenizer, vocab, maxlen, special_tokens, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens \u001b[38;5;241m=\u001b[39m special_tokens\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vocab\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sentence_pairs)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tensor(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mQADataset.get_vocab\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     52\u001b[0m flatten_tokens \u001b[38;5;241m=\u001b[39m questions \u001b[38;5;241m+\u001b[39m answers\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(flatten_tokens)\n\u001b[0;32m---> 54\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|UNK|\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dialogue38/lib/python3.8/site-packages/torchtext/vocab/vocab_factory.py:99\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     97\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# First sort by descending frequency, then lexicographically\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dialogue38/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "special_tokens = {'bos_token': '|BOS|',\n",
    "                  'pad_token': '|PAD|',\n",
    "                  'eos_token': '|EOS|',\n",
    "                  'unk_token': '|UNK|'}\n",
    "\n",
    "train_dataset = load_dataset('knkarthick/dialogsum', split='train')\n",
    "lm_train = QADataset(train_dataset, None, None, 100, special_tokens, 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  285,    5,  207,  565,    4,    6,   34, 1899,    3,    4,  139,\n",
       "          23,    7,   78,  173,    9,    1,    6,  511,   13,   69,   25,   11,\n",
       "          68,  218,   10,   53,   11,  250,   56,   85,    4,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration knkarthick--dialogsum-b0174fca0a26ed84\n",
      "Found cached dataset csv (/home/sefa/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-b0174fca0a26ed84/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Using custom data configuration knkarthick--dialogsum-b0174fca0a26ed84\n",
      "Found cached dataset csv (/home/sefa/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-b0174fca0a26ed84/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda'\n",
    "MAXLEN = 50\n",
    "EPOCHS = 10\n",
    "BS = 64\n",
    "LR = 0.01\n",
    "\n",
    "N_EMBED = 256\n",
    "N_LAYERS = 2\n",
    "N_HIDDEN = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "special_tokens = {'bos_token': '|BOS|', \n",
    "                  'eos_token': '|EOS|',\n",
    "                  'pad_token': '|PAD|',\n",
    "                  'unk_token': '|UNK|',\n",
    "                  'mask_token': '|MASK|'}\n",
    "\n",
    "def create_tokenized_pairs(x):\n",
    "    sentences = re.split(r'[\\s]*#Person\\d#: ', x['dialogue'])[1:]\n",
    "    \n",
    "    sentences1, sentences2 = [], []\n",
    "    for sent1, sent2 in zip(sentences[:-1], sentences[1:]):\n",
    "        sent1 = ' '.join([special_tokens['bos_token'], sent1, special_tokens['eos_token']])\n",
    "        sent2 = ' '.join([special_tokens['bos_token'], sent2, special_tokens['eos_token']])\n",
    "        \n",
    "        sent1 = tokenizer(sent1)[:MAXLEN]\n",
    "        sent2 = tokenizer(sent2)[:MAXLEN]\n",
    "        \n",
    "        sent1 += [special_tokens['pad_token']] * (MAXLEN - len(sent1))\n",
    "        sent2 += [special_tokens['pad_token']] * (MAXLEN - len(sent2))\n",
    "        \n",
    "        sentences1.append(sent1)\n",
    "        sentences2.append(sent2)\n",
    "        \n",
    "    return {'sent1': sentences1, 'sent2': sentences2}\n",
    "\n",
    "tokenizer = get_tokenizer('spacy')\n",
    "\n",
    "train_dataset = load_dataset('knkarthick/dialogsum', split='train')\n",
    "val_dataset = load_dataset('knkarthick/dialogsum', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167fe0715fdb40e7831e1d5eb42248d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## vocab from iterator\n",
    "tokenized_dataset = train_dataset.map(create_tokenized_pairs)\n",
    "\n",
    "flatten_sent1 = [sent for sents in tokenized_dataset['sent1'] for sent in sents]\n",
    "flatten_sent2 = [sents[-1] for sents in tokenized_dataset['sent2']]\n",
    "flatten_sents = flatten_sent1 + flatten_sent2\n",
    "\n",
    "vocab = build_vocab_from_iterator(flatten_sents, min_freq=5, specials=list(special_tokens.values()))\n",
    "vocab.set_default_index(vocab[special_tokens['unk_token']])\n",
    "vocab.set_default_index(vocab[special_tokens['mask_token']])\n",
    "del flatten_sent1\n",
    "del flatten_sent2\n",
    "del flatten_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(LMDataset, self).__init__()\n",
    "\n",
    "        dataset = dataset.map(create_tokenized_pairs)\n",
    "        self.flatten_sent1 = [vocab(sent) for sents in dataset['sent1'] for sent in sents]\n",
    "        self.flatten_sent1 = torch.tensor(self.flatten_sent1)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.flatten_sent1[i]\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.flatten_sent1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling with BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e89e28709c4edc836e354838d59980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85473bf23b1447eabc5eace09f5547d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_train = LMDataset(train_dataset)\n",
    "lm_val = LMDataset(val_dataset)\n",
    "\n",
    "train_loader = DataLoader(lm_train, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(lm_val, batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, n_layers=32, n_hidden=128):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[special_tokens['pad_token']])\n",
    "        self.bilstm = nn.LSTM(embed_dim, n_hidden, num_layers=n_layers, \n",
    "                            dropout=DROPOUT_RATE, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embed(x))\n",
    "        x, (hx, cx) = self.bilstm(x)\n",
    "        h_fold = hx[:2, :, :] + torch.flip(hx[2:, :, :], dims=[-1])\n",
    "        c_fold = cx[:2, :, :] + torch.flip(cx[2:, :, :], dims=[-1])\n",
    "        return x, (h_fold, c_fold)\n",
    "\n",
    "    \n",
    "class MLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, n_layers=32, n_hidden=128):\n",
    "        super(MLM, self).__init__()\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(vocab_size, embed_dim=embed_dim, \n",
    "                                     n_layers=n_layers, n_hidden=n_hidden)\n",
    "        self.fc = nn.Linear(2*n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (hx, cx) = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(ids):\n",
    "    pad_id = vocab.lookup_indices([special_tokens['pad_token']])[0]\n",
    "    mask_id = vocab.lookup_indices([special_tokens['mask_token']])[0]\n",
    "    \n",
    "    rand_ids = torch.zeros_like(ids, dtype=bool)\n",
    "    new_ids = torch.empty_like(ids).copy_(ids)\n",
    "    \n",
    "    for i, sent in enumerate(ids):\n",
    "        max_index = sent.tolist().index(pad_id)+1 if pad_id in sent else MAXLEN\n",
    "        rand_id = torch.randint(1, max_index, (1,)).item()\n",
    "        rand_ids[i, rand_id] = 1\n",
    "        new_ids[i, rand_id] = mask_id\n",
    "\n",
    "    return new_ids, rand_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3eaf37ad9946ada279c1eb767cc708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74070b9f594439dafb6476743d3b9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2c3a58818d47e8bf52d978f095c051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7f83dd165a4547a0f17a207de02434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8378e1877fa744bf963413cd99fee452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839a6a7f53b544f1b3fe6959a8d84cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ed0cf301014a2ea623174f431aa19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43909fff6c6346a1b299fb770e089426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361de7079c304c1bb6838dc31ff7a721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1208028a9841f682eeb2df87639178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d633224661245b4880b91e8e1706105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90df0d87e5f244038b62e5a6b41ea0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebff7ac1de004fa08f330cf25ff7eab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cae0da37f82415294054358c6e8fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63586fc3e9d455291c810deec81501f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320e047d99894c728488290c9ef6416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b92fcfb5b9b4bf7a1c2af1fc4b79855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41ab7c47bb3424891afac8f0dc97183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62155e6824904624b44f5d8b25c253c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20894daa0f245539f7b720b4511322e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about']\n"
     ]
    }
   ],
   "source": [
    "input_sent = ' '.join(['Hi, how', special_tokens['mask_token'],  'you?'])\n",
    "input_sent = ' '.join([special_tokens['bos_token'], input_sent, special_tokens['eos_token']])\n",
    "input_sent = vocab(tokenizer(input_sent))\n",
    "input_sent = torch.tensor([input_sent], device=DEVICE)\n",
    "\n",
    "lm_head = MLM(len(vocab), embed_dim=N_EMBED, n_layers=N_LAYERS, n_hidden=N_HIDDEN).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(lm_head.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    lm_head.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    train_losses = []\n",
    "    for batch_idx, x in enumerate(pbar):\n",
    "        x = x.to(DEVICE)\n",
    "        y = x.to(DEVICE)\n",
    "        \n",
    "        x, ids = random_mask(x)\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        y_pred = lm_head(x)\n",
    "        loss = criterion(y_pred, y[ids])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lm_head.parameters(), 0.9)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_description(f'Loss: {np.mean(train_losses):.3f}')\n",
    "    \n",
    "        #wandb.log({'epochs': epoch,\n",
    "        #           'learning_rate': LR,\n",
    "        #           'loss': loss.item()})\n",
    "    \n",
    "    lm_head.eval()\n",
    "    pbar = tqdm(val_loader)\n",
    "    val_losses = []\n",
    "    for batch_idx, x in enumerate(pbar):\n",
    "        x = x.to(DEVICE)\n",
    "        y = x.to(DEVICE)\n",
    "        \n",
    "        x, ids = random_mask(x)\n",
    "        \n",
    "        y_pred = lm_head(x)\n",
    "        loss = criterion(y_pred, y[ids])\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "        pbar.set_description(f'Loss: {np.mean(val_losses):.3f}')\n",
    "    \n",
    "    # on epoch end\n",
    "    out = lm_head(input_sent)\n",
    "    tokens = vocab.lookup_tokens(out.argmax(dim=-1).tolist())\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = lm_head.encoder\n",
    "torch.save(encoder, 'bilstm_encoder.pt')\n",
    "# lm_head = torch.load('bilstm_encoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Generation Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(DialogueDataset, self).__init__()\n",
    "        \n",
    "        dataset = dataset.map(create_tokenized_pairs)\n",
    "        self.flatten_sent1 = [vocab(sent) for sents in dataset['sent1'] for sent in sents]\n",
    "        self.flatten_sent2 = [vocab(sent) for sents in dataset['sent2'] for sent in sents]\n",
    "        \n",
    "        self.flatten_sent1 = torch.tensor(self.flatten_sent1)\n",
    "        self.flatten_sent2 = torch.tensor(self.flatten_sent2)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.flatten_sent1[i], self.flatten_sent2[i]\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.flatten_sent1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc7c432e6f940b5bed178b65730d186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095dc93045894a6c841543ee20551693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialog_train = DialogueDataset(train_dataset)\n",
    "dialog_valid = DialogueDataset(val_dataset)\n",
    "\n",
    "train_loader = DataLoader(dialog_train, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(dialog_valid, batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, n_layers=32, n_hidden=128):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[special_tokens['pad_token']])\n",
    "        self.lstm = nn.LSTM(embed_dim, n_hidden, num_layers=n_layers, \n",
    "                            dropout=DROPOUT_RATE, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        \n",
    "    def forward(self, x, states):\n",
    "        x = self.dropout(self.embed(x))\n",
    "        out, states = self.lstm(x, states)\n",
    "        out = self.fc(out)\n",
    "        return out, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d341514ca2422c9348b73bca372a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| This 's 's . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a226a50dd446c58823afab7398588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc82303e149746408f38dd6a465d5f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here\n",
      "ANSWER:  |BOS| |BOS| |BOS| coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons coupons\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce03f05503d4cd79cb04c9a0f33100c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c658c3c3bab441b295f82702886b068e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "ANSWER:  |BOS| |BOS| |BOS| at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at\n",
      "ANSWER:  |BOS| |BOS| |BOS| at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at\n",
      "ANSWER:  |BOS| |BOS| at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new\n",
      "ANSWER:  |BOS| |BOS| |BOS| one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah Yeah\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34801d7523844150a306eb4ff18af20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e02369b9ed486abf603514ce149f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one\n",
      "ANSWER:  |BOS| |BOS| |BOS| anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure sure\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8953d781ef1f4e3baa8847fff095cef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f92a8a3c234414b48892f528b670a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities universities\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan yuan\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing Nothing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea66ede29fc4e5c9650b24e64a7ebe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d2034700c3489d95374b1fdbfb3f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others Others\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked booked\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all\n",
      "ANSWER:  |BOS| |BOS| |BOS| term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term term\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101e4335c9704cab81730cd7741b99fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f67a155f6be4fdcb65b5c27115c3234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells\n",
      "ANSWER:  |BOS| |BOS| Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks\n",
      "ANSWER:  |BOS| |BOS| |BOS| bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells bells\n",
      "ANSWER:  |BOS| |BOS| Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming\n",
      "ANSWER:  |BOS| |BOS| No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f03e9d08a9e4faa8dfc3c721f092a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797d11ba87234f7da539267353de5b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks Thanks\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming swimming\n",
      "ANSWER:  |BOS| |BOS| |BOS| aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren aren\n",
      "ANSWER:  |BOS| |BOS| bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies bodies\n",
      "ANSWER:  |BOS| |BOS| midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight midnight\n",
      "ANSWER:  |BOS| |BOS| |BOS| shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter shorter\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b84ffac6e344fda8b438d1cdcb4f02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a8b5d06ad1497cb98147b80efd09d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian Brian\n",
      "ANSWER:  |BOS| |BOS| |BOS| , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious superstitious\n",
      "ANSWER:  |BOS| |BOS| |BOS| robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots robots\n",
      "ANSWER:  |BOS| |BOS| |BOS| as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as\n",
      "ANSWER:  |BOS| |BOS| |BOS| caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused caused\n",
      "ANSWER:  |BOS| |BOS| |BOS| Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum Museum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad90edee1a084473a8f9760c8741aea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46389d999f6468ea6875a435f147ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  |BOS| |BOS| |BOS| Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden Sweden\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more more\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients patients\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle tackle\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets gets\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n",
      "ANSWER:  |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS| |BOS|\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260879beeb58401fad65c8e9d1517d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## training loop\n",
    "#wandb.init('lstm_tuner', project='chat2learn', config={'batch_size': BS, \n",
    "#                                                       'learning_rate': LR, \n",
    "#                                                       'epochs': EPOCHS})\n",
    "input_sent = 'Hi, how are you?'\n",
    "input_sent = ' '.join([special_tokens['bos_token'], input_sent, MAXLEN * special_tokens['eos_token']])\n",
    "input_sent = vocab(tokenizer(input_sent)[:MAXLEN])\n",
    "input_sent = torch.tensor([input_sent], device=DEVICE)\n",
    "\n",
    "encoder = torch.load('bilstm_encoder.pt')\n",
    "dialog_head = LSTMDecoder(len(vocab), embed_dim=N_EMBED, n_layers=N_LAYERS, \n",
    "                          n_hidden=N_HIDDEN).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[special_tokens['pad_token']])\n",
    "optimizer = Adam(dialog_head.parameters(), lr=LR)\n",
    "\n",
    "#encoder.eval()\n",
    "\n",
    "min_val_loss = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(train_loader)\n",
    "    train_losses = []\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        encoder.eval(), dialog_head.train()\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()        \n",
    "        _, states = encoder(x)\n",
    "        states = [state.detach() for state in states]\n",
    "        y_pred, states = dialog_head(y, states)\n",
    "        loss = criterion(y_pred.moveaxis(1, -1), y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dialog_head.parameters(), 0.9)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_description(f'Loss: {np.mean(train_losses):.3f}')\n",
    "    \n",
    "        #wandb.log({'epochs': epoch,\n",
    "        #           'learning_rate': LR,\n",
    "        #           'loss': loss.item()})\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            encoder.eval(), dialog_head.eval()\n",
    "            out_word = vocab(tokenizer(special_tokens['bos_token']))\n",
    "            out_word = torch.tensor([out_word], device=DEVICE)\n",
    "            \n",
    "            _, states = encoder(input_sent)\n",
    "\n",
    "            answer = [special_tokens['bos_token']]\n",
    "            for i in range(MAXLEN):\n",
    "                pred, states = dialog_head(out_word, states)\n",
    "                out_word = pred.argmax(dim=-1)\n",
    "                out_token = vocab.lookup_token(out_word)\n",
    "                \n",
    "                answer.append(out_token)\n",
    "                if answer[-1] == special_tokens['eos_token']:\n",
    "                    break\n",
    "                    \n",
    "            print('ANSWER: ', ' '.join(answer))\n",
    "\n",
    "    val_losses = []\n",
    "    encoder.eval(), dialog_head.eval()\n",
    "    pbar = tqdm(val_loader)\n",
    "    for x, y in pbar:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        _, states = encoder(x)\n",
    "        y_pred, states = dialog_head(y, states=states)\n",
    "        loss = criterion(y_pred.moveaxis(1, -1), y)\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        pbar.set_description(f'Validation Loss: {np.mean(val_losses):.3f}')\n",
    "        \n",
    "        # curr_lr = scheduler.get_last_lr()[-1]\n",
    "        #wandb.log({'epochs': epoch, 'learning_rate': LR, 'val_loss': loss.item()})\n",
    "    \n",
    "    #if min_val_loss is None or np.mean(val_losses) < min_val_loss:\n",
    "    #    min_val_loss = np.mean(val_losses)\n",
    "    #    torch.save(model, 'lstm_model.pt')\n",
    "\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(dialog_head, 'lstm_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(vocab.lookup_tokens(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(vocab.lookup_tokens(y_pred.argmax(dim=-1)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3213336666666666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1.306264, 1.341686, 1.316051])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
